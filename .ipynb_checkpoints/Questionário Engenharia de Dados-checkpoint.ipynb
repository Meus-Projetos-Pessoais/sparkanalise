{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Qual o objetivo do comando <b>cache</b> em Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "        É uma função que armazena em memória resultados provisórios para que sejam usados de forma subsequente otimizando os cálculos no spark,principalmente quando as ações são repetitivas. \n",
    "    \n",
    "    \n",
    "<b>Fonte:</b>\n",
    "<a href= 'https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd-caching.html'>RDD Caching and Persistence</a>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - O mesmo código implementado em Spark é normalmente mais rápido que a implementação equivalente em\n",
    "MapReduce. Por quê?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Utilizei como base o comparativo escrito por Marcelo Costa, baseado no artigo \"Clash of the Titans: MapReduce vs. Spark for Large ScaleData Analytics\".\n",
    "\n",
    "Respondendo de forma direta, sim é mais rápido, devido o carregamento de dados ser feito em memória, a leitura dos dados serem feitas de forma distribuida e utiliza cache para facilitar, o que faz com que trabalhos iterativos sejam mais fáceis. E de acordo com o comparativo citado, as vantagens elencadas são: \n",
    "\n",
    "\" 1 - Para operações de Word Count e cargas de trabalho semelhantes, em que a seletividade da saída do map pode ser significativamente reduzida por meio de uma combinação no lado do map, a agregação com base em hash no Spark é mais eficiente comparado com a agregação baseada em Sort do MapReduce. O resultado para o tempo de execução medido indica que o framework baseado em hash contribui cerca de 39% da melhoria global para o Spark;\n",
    "\n",
    " 2 - Para os algoritmos iterativos, tais como: k-means e PageRank, deixando as entradas em cache RDD pode reduzir tanto a sobrecarga de CPU (conversão de texto para objetos) quanto a sobrecarga de I/O em disco para as iterações subsequentes. Vale ressaltar que a sobrecarga da CPU é muitas vezes o gargalo em cenários nos quais as iterações subsequentes não usam o cache RDD. Como resultado, o cache RDD é muito mais eficiente comparado a outras abordagens de cache de baixo nível, tais como: OS buffer cache e cache em HDFS, que só são capazes de reduzir o I/O em disco. Por meio de pequenos experimentos, foi identificado que reduzir a sobrecarga de parse em CPU contribui em mais de 90% no aumento da velocidade geral para as iterações subsequentes em k-means;\n",
    " \n",
    " 3 - Uma vez que o Spark permite o pipeline de dados dentro de uma fase, isto evita sobrecarga da materialização na saída dos dados no HDFS (serialização, I/O em disco e I/O em rede) entre iterações nos algorítimos de análise de grafo como o PageRank. Uma exceção à vantagem em desempenho do Spark sobre o MapReduce é para cargas de trabalho do tipo Sort, no qual o MapReduce é 2x mais rápido que o Spark. Isto ocorre devido as diferenças entre os planos de execução de tarefas. O MapReduce pode sobrepor a fase de shuffle com a fase de map, o que efetivamente esconde a sobrecarga da rede que muitas vezes é um gargalo para a fase de reduce.\"\n",
    " \n",
    "Entendo que há mais situações a analisar, ainda mais que quando em disco, a persistência do dado pode ser atrativo, creio que uma proposta de juntar o melhor dos dois ou analisar mais situações para empregar a solução tem que ser conversada, pricinpalmente com quem trabalha com isso e com mais ideias, para assim ter uma infraestrutura mais rápida, escalável e disponível.\n",
    "\n",
    "Uma pessoa que gosto muito de ver é Ricardo Paixa, me deu muitas ideias sobre big data e processamento altamente massivo, que ouvi mais no podcast Pizza de Dados, mas já o conheço tem um tempo.\n",
    "<a href='https://podcast.pizzadedados.com/e/episodio-024/'>Episódio 024: Engenharia de Dados… e põe dados nisso!</a>\n",
    "\n",
    "<b>Fontes:</b>\n",
    "<a href =  'https://www.infoq.com/br/articles/mapreduce-vs-spark/'>Um comparativo entre MapReduce e Spark para analise de Big Data </a>\n",
    "<a href ='http://www.vldb.org/pvldb/vol8/p2110-shi.pdf'>Clash of the Titans: MapReduce vs. Spark for Large ScaleData Analytics</a>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - Qual é a função do <b>SparkContext</b>?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\n",
    "O Spark context, é o ponto de entrada de acessos ao serviço Spark. É nesse ponto que as configurações são feitas para acesso  ao RDD, acumuladores até executar as tarefas no Spark, atuando de como se fosse um orquestrador(mestre)para o processamento de dados.\n",
    "\n",
    "<b>Fontes</b>\n",
    "<a href =  'https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-SparkContext.html'>SparkContext — Entry Point to Spark Core</a>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - Explique com suas palavras o que é <b>Resilient Distributed Datasets (RDD)</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "O RDD é uma forma de distribuição de dados entre os nós dos sistema, é uma forma de entender e dispersar,s os dados estão espalhados no cluster. O \"Resilient Distributed\" dessa sigla é o entendimento que sempre haverá dados, pois eles estão distribuidos e são de fácil acesso,mantendo assim a disponibilidade.\n",
    "\n",
    "<b>Fontes :</b>\n",
    "<a href = 'http://www.inf.ufpr.br/erlfilho/tutorials/spark/#RDDs'>Abstrações de dados</a>\n",
    "<a href = 'https://www.oreilly.com/learning/what-is-a-resilient-distributed-dataset'>What is a resilient distributed dataset?</a>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5- <b>GroupByKey</b> é menos eficiente que <b>reduceByKey</b> em grandes dataset. Por quê?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 - Explique o que o código Scala abaixo faz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"codigoscala.png\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\n",
    "Esse código serve para separar as palavras em string e int, fazendo o mapeamento e a redução dimensional das palavras do conjunto de dados\n",
    "\n",
    "Um exemplo que pode ser feito é o seguinte com as frases abaixo:\n",
    "\n",
    "1 - Eu gosto de chocolate\n",
    "\n",
    "2 - Eu gosto de jujubas.\n",
    "\n",
    "Fazendo o mapeamento das duas frases, teremos o seguinte resultado.\n",
    "\n",
    "ID | |01  | | 02     | |03  | |04           |\n",
    "1  | |Eu,1| |gosto,1 | |de,1| |chocolate, 1 |\n",
    "2  | |Eu,1| |gosto,1 | |de,1| | jujuba , 1  |\n",
    "\n",
    "Contamos todas as palavras, e agora iremos reduzir, pois há repetições de palavras.\n",
    "\n",
    "ID  | |01   | |02     | |03  |\n",
    "1,2 | |Eu,2 | |gosto,2| |de,2|\n",
    "\n",
    "ID| |04           |\n",
    "1 | |chocolate, 1 |\n",
    "2 | |jujuba , 1   |\n",
    "\n",
    "Pelo índice da frase, temos mapeados e reduzidos as palavras, evitando repetição, e mais a abaixo, as palavras que não se repetem para serem ligadas à frase principal. Isso ajuda no processamento, pois diminui a dimensão do bloco de dados a ser análisado.\n",
    "\n",
    "\n",
    "\n",
    "<b>Fontes :</b>\n",
    "<a href =  'https://spark.apache.org/examples.html'>Apache Spark Examples</a> \n",
    "<a href =  'http://www.univale.com.br/unisite/mundo-j/artigos/53_Mapreduce.pdf'>Funcionamento e Recursos MapReduce Detalhado</a>\n",
    "<a href =  'https://www.ime.usp.br/~ipolato/JAI2012-Hadoop.pdf'>Apache Hadoop:conceitos teóricos e práticos,evolução e novas possibilidades.</a>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<pre>\n",
    "<b>Fonte:</b>\n",
    "<a href = 'https://github.com/darlansf1/SPark'>Repositório do Github de Darlan Santana Farias</a>\n",
    "</pre>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
